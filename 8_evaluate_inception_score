name: Evaluate Inception Score
description: Calculate Inception Score (IS) for generated images. Measures both quality and diversity. Higher IS indicates better quality and more diverse generated images. Formula exp(E[KL(p(y|x) || p(y))]) measures prediction confidence (quality) vs marginal distribution (diversity).

inputs:
  - name: images
    type: Data
    description: 'Numpy file with generated images from Generate Images component'
  - name: model_id
    type: String
    description: 'Model identifier for tracking (e.g., "CompVis/stable-diffusion-v1-4")'
  - name: batch_size
    type: String
    description: 'Batch size for processing images'
    default: '32'
  - name: splits
    type: String
    description: 'Number of splits for computing mean and std'
    default: '10'
  - name: device
    type: String
    description: 'Device: cuda or cpu'
    default: 'cuda'

outputs:
  - name: result
    type: Data
    description: 'JSON file with Inception Score and evaluation metadata'

metadata:
  annotations:
    author: Evaluate Inception Score Component

implementation:
  container:
    image: kushagra4761/nesy-factory-gpu:t2.6v2
    command:
      - sh
      - -c
      - |
        set -e
        echo "Installing Inception Score evaluation dependencies..."
        pip install --no-cache-dir \
          torchmetrics>=1.0.0 \
          torch-fidelity>=0.3.0 \
          scipy>=1.10.0 \
          --break-system-packages > /dev/null 2>&1
        
        echo "Dependencies installed. Starting component..."
        echo ""
        
        "$0" "$@"
      - python3
      - -u
      - -c
      - |
        def _make_parent_dirs_and_return_path(file_path):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path
        
        def evaluate_inception_score_component(
            images_input_path,
            model_id,
            batch_size,
            splits,
            device,
            result_output_path,
        ):
            import os
            import sys
            import json
            import logging
            import numpy as np
            import torch
            from datetime import datetime
            
            logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            logger = logging.getLogger('evaluate_inception_score')
            
            def ensure_directory_exists(file_path):
                directory = os.path.dirname(file_path)
                if directory and not os.path.exists(directory):
                    os.makedirs(directory, exist_ok=True)
                    logger.info(f'Created directory: {directory}')
            
            def calculate_inception_score(imgs, dev, batch_sz, num_splits):
                # Calculate Inception Score
                # Args:
                #   imgs: numpy array of images (N, H, W, C) in [0, 1]
                #   dev: cuda or cpu
                #   batch_sz: batch size for processing
                #   num_splits: number of splits for computing mean/std
                # Returns:
                #   mean_score, std_score
                from torchmetrics.image.inception import InceptionScore
                
                logger.info('Computing Inception Score...')
                logger.info(f'  Images: {len(imgs)}')
                logger.info(f'  Splits: {num_splits}')
                
                # Setup metric
                inception_metric = InceptionScore(normalize=True).to(dev)
                
                # Convert images to tensor
                if isinstance(imgs, np.ndarray):
                    images_tensor = torch.from_numpy(imgs).float()
                    # Ensure NCHW format
                    if images_tensor.dim() == 4 and images_tensor.shape[-1] in [1, 3]:
                        images_tensor = images_tensor.permute(0, 3, 1, 2)
                else:
                    images_tensor = imgs
                
                # Ensure [0, 255] range for Inception
                if images_tensor.max() <= 1.0:
                    images_tensor = images_tensor * 255
                
                images_tensor = images_tensor.to(torch.uint8)
                
                logger.info(f'  Image tensor shape: {images_tensor.shape}')
                logger.info(f'  Image tensor range: [{images_tensor.min()}, {images_tensor.max()}]')
                
                # Process in batches
                for i in range(0, len(images_tensor), batch_sz):
                    batch = images_tensor[i:i+batch_sz].to(dev)
                    inception_metric.update(batch)
                    
                    if (i // batch_sz + 1) % 10 == 0:
                        logger.info(f'  Processed {min(i+batch_sz, len(images_tensor))}/{len(images_tensor)}')
                
                # Compute score
                logger.info('  Computing final score...')
                mean_score, std_score = inception_metric.compute()
                
                return float(mean_score), float(std_score)
            
            # Parse parameters
            batch_sz = int(batch_size)
            num_splits = int(splits)
            
            logger.info('='*80)
            logger.info('EVALUATE INCEPTION SCORE COMPONENT')
            logger.info('='*80)
            logger.info(f'Images: {images_input_path}')
            logger.info(f'Model ID: {model_id}')
            logger.info(f'Batch size: {batch_sz}')
            logger.info(f'Splits: {num_splits}')
            logger.info(f'Device: {device}')
            logger.info('')
            
            try:
                ensure_directory_exists(result_output_path)
                
                # Validate inputs
                if not os.path.exists(images_input_path):
                    raise FileNotFoundError(f'Images file not found: {images_input_path}')
                
                # Load images
                logger.info(f'Loading images from {images_input_path}...')
                images = np.load(images_input_path)
                logger.info(f'✓ Loaded {len(images)} images with shape {images.shape}')
                
                # Ensure [0, 1] range
                if images.max() > 1.0:
                    images = images / 255.0
                    logger.info('  Normalized to [0, 1] range')
                
                # Validate minimum samples
                if len(images) < 100:
                    logger.warning(f'Inception Score requires many samples (100+) for reliable results')
                    logger.warning(f'  Current: {len(images)} samples')
                
                logger.info('')
                logger.info('Evaluating Inception Score...')
                
                # Calculate IS
                mean_score, std_score = calculate_inception_score(
                    imgs=images,
                    dev=device,
                    batch_sz=batch_sz,
                    num_splits=num_splits
                )
                
                logger.info('')
                logger.info('='*80)
                logger.info(f'Inception Score: {mean_score:.4f} ± {std_score:.4f}')
                logger.info('='*80)
                logger.info('')
                logger.info('Interpretation (higher is better):')
                logger.info('  > 10: Excellent quality and diversity')
                logger.info('  5-10: Good')
                logger.info('  < 5: Poor')
                logger.info('')
                logger.info('Note: IS is best compared between models on same dataset')
                logger.info('')
                
                # Save result
                result = {
                    'metric_name': 'inception_score',
                    'metric_value': mean_score,
                    'std': std_score,
                    'model_id': model_id,
                    'num_samples': len(images),
                    'splits': num_splits,
                    'timestamp': datetime.now().isoformat(),
                    'metadata': {
                        'batch_size': batch_sz,
                        'device': device,
                        'image_shape': list(images.shape)
                    }
                }
                
                with open(result_output_path, 'w') as f:
                    json.dump(result, f, indent=2)
                
                logger.info(f'✓ Saved result to {result_output_path}')
                
                logger.info('')
                logger.info('='*80)
                logger.info('INCEPTION SCORE EVALUATION COMPLETED')
                logger.info('='*80)
                logger.info(f'IS Score: {mean_score:.4f} ± {std_score:.4f}')
                logger.info(f'Model: {model_id}')
                logger.info(f'Samples: {len(images)}')
                logger.info('='*80)
                
            except FileNotFoundError as e:
                logger.error(f'FILE NOT FOUND ERROR: {str(e)}')
                import traceback
                traceback.print_exc()
                sys.exit(1)
            except ValueError as e:
                logger.error(f'VALIDATION ERROR: {str(e)}')
                import traceback
                traceback.print_exc()
                sys.exit(1)
            except Exception as e:
                logger.error(f'ERROR: {str(e)}')
                import traceback
                traceback.print_exc()
                sys.exit(1)
        
        import argparse
        _parser = argparse.ArgumentParser(prog='Evaluate Inception Score', description='Calculate Inception Score for generated images')
        _parser.add_argument('--images', dest='images_input_path', type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument('--model_id', dest='model_id', type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument('--batch_size', dest='batch_size', type=str, required=False, default='32')
        _parser.add_argument('--splits', dest='splits', type=str, required=False, default='10')
        _parser.add_argument('--device', dest='device', type=str, required=False, default='cuda')
        _parser.add_argument('--output_result', dest='result_output_path', type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())
        evaluate_inception_score_component(**_parsed_args)

    args:
      - --images
      - {inputPath: images}
      - --model_id
      - {inputValue: model_id}
      - --batch_size
      - {inputValue: batch_size}
      - --splits
      - {inputValue: splits}
      - --device
      - {inputValue: device}
      - --output_result
      - {outputPath: result}
